{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93f77938-7669-4c71-aabe-49ecd7316e2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "# Taxi Trip Fare Prediction - Model 3\n",
    "\n",
    "***\n",
    "\n",
    "The goal of this example is to build on the Model 2 example and add real-time features from the Events Data Sink. We will\n",
    "- train an ML model based on historical trip fare Events Data Sink and the three Static Data Sinks\n",
    "- add real-time window aggregate features calculated on real-time streaming data to the Events Data Sink\n",
    "- serve the ML model to predict the trip fare for new trips\n",
    "\n",
    "### Prepare your data\n",
    "\n",
    "In the Model 1 example we used trip table from an S3 bucket to configure a data source and a data sink.\n",
    "\n",
    "### Prepare your static contextual feature data\n",
    "\n",
    "In the Model 2 example we enhanced the data by adding three static data sinks from three data sources. \n",
    "\n",
    "- an hourly segment table that maps an hour to an hourly_segment. \n",
    "- a holiday weekend table that maps a date to a flag indicating whether that date was a holiday-or-weekend or neither.\n",
    "- a geo area table that maps a zipcode to a type of geo area.\n",
    "\n",
    "### Prepare your real-time contextual feature data\n",
    "\n",
    "In this example we will add a contextual feature that is a window time aggregated function on the Events Data Sink.\n",
    "\n",
    "- the total passenger count for a zipcode within the last 4 hours\n",
    "\n",
    "The idea is that the total passenger count for all trips starting at a zipcode within the last 4 hours, indicates the recent demand at a zipcode and has an influence on the trip fare amount. We can create a more accurate ML model with this additional feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976afc32-8c6f-4e66-a4fc-819d702f7b2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "**We will reuse the `trip_fare` project from Model 1 for this example.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775400e0-ee27-498e-9015-6ed4622c98fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set project trip_fare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664147e-01c5-4fd6-9ffe-1c1fffed8238",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "# Configure Data Sources\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_1.png\"/></html>\n",
    "\n",
    "In the Model 1 example we have configured the trip table csv file from an S3 bucket as a data source to Aizen. In the Model 2 example we have configured three more data sources corresponding to the three static csv files in the S3 bucket. Model 3 has no further data sources, so there is nothing to do for this step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54851315-21ef-4264-9999-a14686f07bfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "# Configure Data Sinks\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_2.png\"/></html>\n",
    "\n",
    "In the Model 1 example we have configured an Events Data Sink against the trip table data source. In the Model 2 example we have configured three Static Data Sinks to the three data sources providing contextual data. Model 3 has no further data sources or data sinks, so there is nothing to do for this step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e5d1e-d9db-4a76-8695-12edb92848a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "# Create a Training Dataset\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_3.png\"/></html>\n",
    "\n",
    "In this step we will create a training dataset from the data sinks. In the Model 1 and Model 2 examples, we added the pickup_zipcode, dropoff_zipcode, passenger_count, hourly_segment, is_holiday_or_weekend, pickup_geo_area and dropoff_geo_area as input features to the ML model. The fare_amount is the target or label for the ML model to train and was added as a label feature. All features were drawn from the Events Data Sink and the three Static Data Sinks.\n",
    "<br>Additionally, for Model 3 we will define a contextual feature 'total_passenger_count_4hr' from the Events Data Sink. This feature will be defined as the sum of the passenger_count over the last 4 hours for a given pickup zipcode.\n",
    "\n",
    "## Building Datasets from Data Sinks\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_4.png\"/></html>\n",
    "\n",
    "<br>Basis features are sourced from a single data sink. Contextual features are retrieved from data sinks using join keys from the basis features.\n",
    "\n",
    "Datasets are configured via the `configure dataset` command. This command will prompt for various settings. The relevant information for this command is shown below. Enter this information in the prompts:\n",
    "    \n",
    "            Dataset: New                  Dataset Name: trip_dataset_3\n",
    "            Feature: Create New\n",
    "            Feature Type: Basis\n",
    "            Data Sink: trip_datasink\n",
    "            Feature: pickup_datetime\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "           \n",
    "Click the `Add Feature` button to add the pickup_datetime input feature. Continue to add all features with the following information in the prompts:\n",
    "\n",
    "            Dataset: New\n",
    "            Feature: Create New\n",
    "            Feature Type: Basis\n",
    "            Data Sink: trip_datasink\n",
    "            Feature: pickup_zipcode\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "           \n",
    "Click the `Add Feature` button to add the pickup_zipcode input feature.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Basis\n",
    "            Data Sink: trip_datasink\n",
    "            Feature: dropoff_zipcode\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "\n",
    "Click the `Add Feature` button to add the dropoff_zipcode input feature.           \n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Basis\n",
    "            Data Sink: trip_datasink\n",
    "            Feature: passenger_count\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "            \n",
    "Click the `Add Feature` button to add the passenger_count input feature.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Basis\n",
    "            Data Sink: trip_datasink\n",
    "            Feature: fare_amount\n",
    "            Is Label: checked (true)      Materialize: checked (true)\n",
    "\n",
    "Click the `Add Feature` button to add the fare_amount output feature.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Expression\n",
    "            Name: pickup_datetime_hour_of_day\n",
    "            Is Label: unchecked (false)   Materialize: unchecked (false)\n",
    "            Expression: checked (true)\n",
    "            Built-in Expression:\n",
    "            Expression: pickup_datetime.getHours()\n",
    "\n",
    "Click the `Add Feature` button to add the pickup_datetime_hour_of_day key.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Expression\n",
    "            Name: pickup_datetime_calendar_day\n",
    "            Is Label: unchecked (false)   Materialize: unchecked (false)\n",
    "            Expression: checked (true)\n",
    "            Built-in Expression:\n",
    "            Expression: pickup_datetime.getCalendarDay()\n",
    "\n",
    "Click the `Add Feature` button to add the pickup_datetime_calendar_day key.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Contextual\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "            Expression: unchecked (false)\n",
    "            Data Sink: hour_of_day_datasink\n",
    "            Value: hourly_segment\n",
    "    Join Key Map: \n",
    "            hour_of_day: pickup_datetime_hour_of_day\n",
    "\n",
    "Click the `Add Feature` button to add the hourly_segment input feature.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Contextual\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "            Expression: unchecked (false)\n",
    "            Data Sink: holiday_weekend_datasink\n",
    "            Entities: calendar_day\n",
    "            Value: is_holiday_or_weekend\n",
    "    Join Key Map: \n",
    "            calendar_day: pickup_datetime_calendar_day\n",
    "        \n",
    "Click the `Add Feature` button to add the is_holiday_or_weekend input feature.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Contextual\n",
    "            Name: pickup_geo_area\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "            Expression: unchecked (false)\n",
    "            Data Sink: geo_area_datasink\n",
    "            Value: geo_area\n",
    "    Join Key Map: \n",
    "            zipcode: pickup_zipcode\n",
    "        \n",
    "Click the `Add Feature` button to add the pickup_geo_area input feature.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Contextual\n",
    "            Name: dropoff_geo_area\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "            Expression: unchecked (false)\n",
    "            Data Sink: geo_area_datasink\n",
    "            Value: geo_area\n",
    "    Join Key Map: \n",
    "            zipcode: dropoff_zipcode\n",
    "        \n",
    "Click the `Add Feature` button to add the dropoff_geo_area input feature.\n",
    "\n",
    "            Feature: Create New\n",
    "            Feature Type: Contextual\n",
    "            Name: total_passenger_count_4hr\n",
    "            Is Label: unchecked (false)   Materialize: checked (true)\n",
    "            Expression: unchecked (false)\n",
    "            Data Sink: trip_datasink\n",
    "            Source Column or Expression: passenger_count\n",
    "            Condition:\n",
    "            Function: sum\n",
    "            Timestamp: pickup_datetime\n",
    "            Window Start: -4h\n",
    "            Window End: 0h\n",
    "    Join Key Map: \n",
    "            pickup_zipcode: pickup_zipcode\n",
    "        \n",
    "Click the `Add Feature` button to add the total_passenger_count_4hr input feature.\n",
    "<br>Click the `Save Configuration` button followed by the `OK` button to configure the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349d4a3-a439-4eb8-9c84-83a5f3615001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configure dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c9e01-ffb1-4b3f-9930-e14948ac272d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configure a Resource to create the Dataset\n",
    "\n",
    "We will configure a resource to run the dataset job that materializes the dataset table. Resources are configured via the `configure resource` command. This command will prompt for various settings. The relevant information for this command is shown below. Enter this information in the prompts:\n",
    "    \n",
    "            Job Type: dataset              Dataset Name: trip_dataset_3           Resource Type: dataset        Select Resource: New     Resource Name: trip_dataset_resource\n",
    "            Select \"Basic Settings\"        Dataset Size: 0-1M rows\n",
    "            \n",
    "Click the `Save Configuration` button to configure the resource for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e018345-08d9-4d6d-988d-64fe168641c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configure resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616c109-7606-4629-880e-0081fd3dd6a9",
   "metadata": {},
   "source": [
    "### Create the Dataset\n",
    "\n",
    "Use the `start dataset` command to materialize the configured dataset into a training dataset table.The `status dataset` command will show the current status of dataset generation; \"RUNNING\", \"COMPLETED\" or \"ERROR\". The `list datasets` command will list the created datasets within a project. The `display dataset` command will display the first few rows of the training dataset.\n",
    "\n",
    "**This command may take up to 10 minutes due to the size of the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f111f8-f5a2-456d-bda9-d2d847f94ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start dataset trip_dataset_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dfb7f-2811-4bc9-ab82-765214bee59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status dataset trip_dataset_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740aeed-884e-4d2f-a01b-2fac41f7748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacdcf3-0d4d-49ee-9602-8d71edb477fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display dataset trip_dataset_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc29bb-2a49-4019-a6c0-7662029996a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data analysis on the dataset\n",
    "\n",
    "Use the `loader` command to load the dataset for visual exploration. Run the `loader` command, click the `Datasets` button and select the `trip_dataset_3` table.\n",
    "\n",
    "Click the `Load Table` button to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259d6de-f90e-4821-97a6-3c25c646354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe323d-8524-4d89-8d5c-b51601bde342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20eae0-9027-4a1a-a225-041328c49443",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "# Train an ML Model\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_5.png\"/></html>\n",
    "\n",
    "In this step we will train an ML model using the training dataset that was created. We will use the pickup_zipcode, dropoff_zipcode, passenger_count, hourly_segment, is_holiday_or_weekend, pickup_geo_area, dropoff_geo_area and total_passenger_count_4hr as input features to the ML model. The fare_amount will be the target or label for the ML model. \n",
    "\n",
    "A Training Experiment must be configured to train a model. Experiments are configured via the `configure training` command. This command will prompt for various settings. We will configure a Deep Learning experiment for Model 3. The relevant information for this command is shown below. Enter this information in the prompts:\n",
    "    \n",
    "            Training Experiment: New                  Experiment Name: trip_dl_exp_3        Model Name: trip_fare_3_dl_model\n",
    "            Select \"Deep Learning\"                    Select \"Basic Settings\"\n",
    "            Dataset: trip_dataset_3                   Select Column: pickup_datetime        Click Remove Input Feature\n",
    "                                                      Select Column: pickup_datetime_hour_of_day       Click Remove Input Feature\n",
    "                                                      Select Column: pickup_datetime_calendar_day      Click Remove Input Feature\n",
    "            Epochs: 15                                Early Stop: 1                         Batch Size: 2048\n",
    "            \n",
    "Click the `Save Configuration` button to save the Deep Learning experiment configuration.           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456d2b5-36b2-479d-a1cf-d2013f1d5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aba791-a8ab-4372-a9ab-f5f5179afc33",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start ML model training\n",
    "\n",
    "Use the `start training` command to run the training experiment. The `status training` command will show the status of the model training. \n",
    "\n",
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd44cf-b9bb-4a86-b453-2f9e274dedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start training trip_dl_exp_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d78dd-68be-4077-990c-2808ca05ecdf",
   "metadata": {},
   "source": [
    "**Click the url shown in the output to open a *TensorBoard* session that displays the training progress and metrics.** After opening the *TensorBoard* url click on the reload button to the top right of the *TensorBoard* page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c3216-da66-44ab-82a6-1c193d50993c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Training could take 10 minutes or more to complete.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7da80-dc71-4a96-9c49-6efe434a4fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status training trip_dl_exp_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d7d63-c67e-40a2-8ec5-7f1bfa09d8e7",
   "metadata": {},
   "source": [
    "#### Note:\n",
    " TensorBoard is only available for Deep Learning models   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3d83d-c34b-4439-a520-826025ef0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list tensorboard \"<model name>,<run_id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbb8b2-2cfa-44f6-9ba1-7dd32f57e406",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Register a trained ML model\n",
    "\n",
    "After the training is complete, the `status training` command will show COMPLETED status. The trained ML model must be registered before it can be used for predictions. The `list trained-models` command will list all the trained models within a project. The `register model` command will register a trained model. The `list registered-models` will list all registered models within a project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febc44d-a773-4e97-8c58-e4baca8ee83c",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### To list all the DL models that have been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620c63b-61ee-43c6-b639-f921711eb5ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list trained-models trip_fare_3_dl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02bcc1-c804-4c94-a2f4-ac618aecb449",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Run this cell to register the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2098e1b-39ed-415a-9ffc-b2f5b38af229",
   "metadata": {},
   "outputs": [],
   "source": [
    "register model trip_fare_3_dl_model,1,PRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420da29-5d8e-42ee-83b6-907e4ca2592a",
   "metadata": {},
   "source": [
    "#### To list all registered models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455cd6c-2bcd-4d97-aee5-720357426ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list registered-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0cb6b6",
   "metadata": {},
   "source": [
    "## Start a real-time streaming event publisher\n",
    "\n",
    "For serving we must calculate the 'total_passenger_count_4hr' feature from a real-time streaming data source of trip events. We will first create a real-time data stream by publishing trip event data. Real-time trip events are published to a kafka stream using the trip table csv file. We will use the same trip table csv file that we used for historical data to publish real-time streaming data, except that the event timestamp which is the pickup_datetime will be altered to the current time. This step is necessary only because we are simulating a test scenario for this tutorial. In the real world you will have real-time sources that are already publishing event data.\n",
    "\n",
    "First we will fetch the trip table csv using wget and peek at a few lines of data from the csv file. The data includes the pickup datetime, pickup latitude, longitude, dropoff latitude, longitude, pickup and dropoff zipcodes, passenger count and fare amount. We will print the first few lines using the `head` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629bc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget \"https://aizen-public.s3.us-west-2.amazonaws.com/trip_fare/trip_table.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c49c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head -n 5 trip_table.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf399c7e",
   "metadata": {},
   "source": [
    "### Publish real-time trip event data\n",
    "\n",
    "We will use kafka as the data source for the real-time trip events. We will publish trip events to a kafka broker. Use `kafka-producer-csv.py` to publish trip events from the trip table csv file to the kafka broker. This program takes each trip event from the trip_table csv file, updates the timestamps to reflect the current time and continuously publishes events to a kafka broker. It is a long running program and must be executed in the background, otherwise it will prevent other notebook cells from being executed until it completes. The `-b` option specifies the URL of the kafka broker. The `-i` option specifies the input csv file name. The `-e` option specifies the duration in minutes to publish events, the default is 15 minutes. The `-g` option obtains the kafka broker address and topic. The `-h` option displays help.\n",
    "\n",
    "Note the `kafka source meta` from the output below. It will be used later to connect the kafka topic to the Aizen platform.\n",
    "\n",
    "The second command below, that starts with an `&`, will publish trip events in the background to the kafka broker for a duration of 60 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bfb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kafka-producer-csv.py -b <kafka server>:9092 -i trip_table.csv -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acf14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "&kafka-producer-csv.py -b <kafka server>:9092 -i trip_table.csv -e 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6b3c3",
   "metadata": {},
   "source": [
    "# Configure and add a Real-Time Data Source\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_6.png\"/></html>\n",
    "\n",
    "In this step we will connect the kafka broker and topic as a data source to Aizen. This will allow Aizen to read events from the kafka topic. Data sources are connected to Aizen via the `configure datasource` command. This command will prompt for various settings. The relevant information for this command is shown below. Enter this information in the prompts:\n",
    "    \n",
    "            Source: New                Source Name: trip_realtime_datasource\n",
    "            Source Description: taxi trip data\n",
    "            Source Type: kafka\n",
    "            Source Format: json\n",
    "            Broker URL: <kafka server>:9092   # replace the '<kafka server>' using the output from the previous two cells\n",
    "            Kafka Topic: <kafka topic>        # replace the '<kafka topic>' using the output from the previous two cells    \n",
    "            Kafka Offset:\n",
    "            Poll Freq:\n",
    "            \n",
    "Click the `Get Columns` button and review the source column schema. \n",
    "<br>Click the `Save Configuration` button to configure the datasource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc95ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configure datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac3a7a",
   "metadata": {},
   "source": [
    "## Add a Real-Time Data Source to a Data Sink\n",
    "\n",
    "In the Model 1 example, we configured the 'trip_datasink' to the 'trip_datasource', which is a Data Source of historical trip events. In this Model, we have configured a kafka Data Source called 'trip_realtime_datasource' which is a Data Source of real-time trip events. We will now alter the 'trip_datasink' and add the 'trip_realtime_datasource' as a real-time source of data. This command will schedule a datasink job to periodically and continuously read real-time data from the kafka source and store it in the Data Sink. The dataset feature 'total_passenger_count_4hr', which was defined on the 'trip_datasink', will now provide materialized aggregates on real-time data from the kafka source. Before adding the real-time data source to the data sink, we will mark an outage window indicating that there is no data from when the backfill ended up to the current time. This will speed up the process of loading data into the data sink. Replace the `end_timestamp` with the current date in the following command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbced03e-f7ae-4779-a3cc-41fd593dff04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alter datasink trip_datasink,add outage_window start_timestamp=\"2022-11-28 00:00:00\",end_timestamp=\"2024-09-04 00:00:00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9872a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alter datasink trip_datasink,\"add source trip_realtime_datasource\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1373c04-3295-4bab-a880-40c3215037c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "status datasink trip_datasink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c510b4eb-2662-41f1-8436-78ad0008267b",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "# Serve an ML Model\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_7.png\"/></html>\n",
    "\n",
    "In this step we will deploy a trained ML model to serve prediction requests. We will deploy the Deep Learning model. A prediction deployment must be configured to deploy a model. Deployments are configured via the `configure prediction` command. This command will prompt for various settings.\n",
    "\n",
    "The relevant information for this command is shown below. Enter this information in the prompts:\n",
    "    \n",
    "            Prediction: New                  Prediction Name: trip_dl_deploy_3        Model Name: trip_fare_3_dl_model       Model Version: 1\n",
    "            Source Type: http\n",
    "           \n",
    "Click the `Save Configuration` button to save the Machine Learning deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ad263-1026-4de5-bbb7-e3fc93f12071",
   "metadata": {},
   "outputs": [],
   "source": [
    "configure prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7e4fd-7687-476c-89ac-53bcf5874f20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the model\n",
    "\n",
    "Use the `start prediction` command to run the deployment. The `status prediction` command will show the status of the model serving. The url shown in the output is the endpoint to which REST prediction request may be sent via `curl` or some other means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8300485-d1a4-4873-a211-a9f7b7e59e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start prediction trip_dl_deploy_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ea953-f669-4613-8db7-c456f46f07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "status prediction trip_dl_deploy_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45075af8-890f-496e-87e9-06c4c7448ac5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict trip fare amounts\n",
    "\n",
    "Use the `test prediction` command to send prediction requests to the deployed model. The command by default uses the last 10 rows from the training dataset and sends those rows in curl prediction requests to the deployed model. The predictions responses are collected and displayed.\n",
    "\n",
    "Note: when you run the start prediction command, a prediction job starts running which deploys the model. You can use the URL in the status prediction to send curl requests to the deployed model. The `test prediction` command outputs an \"Example Curl Request\". Use this Curl request example to send data to the deployed model or integrate the curl request logic into applications which can send prediction requests and interpret prediction responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1944c31f-b166-49a6-b520-f4fa254da9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test prediction trip_dl_deploy_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27c635-09cd-4b31-bcc2-3e76cb800093",
   "metadata": {},
   "source": [
    "## Building Input Features for Predictions\n",
    "\n",
    "<html><img src=\"../../images/trip_fare_images/3_8.png\"/></html>\n",
    "\n",
    "When an application sends a prediction request, the basis input features are present in the prediction request. Any contextual features are fetched from data sinks and appended to the basis features before calling the model for a prediciton. The labels or output features are returned in the prediction response.\n",
    "\n",
    "The cell below is a Markdown cell showing how to run a Curl Request to fetch predictions. Convert the cell into the Code state, then replace the prediction URL in the text below and execute the cell to get a prediction response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5ee87-d628-4ca2-af3a-ee8dfc62b8ac",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "!curl -X POST \">enter the prediction URL here<\" -H \"Content-Type: application/json\" -d '[{\"rest_request_id\": \"prediction_test-1\", \"pickup_datetime\": \"2022-11-12 11:29:05\", \"pickup_zipcode\": \"10069\", \"dropoff_zipcode\": \"10107\", \"passenger_count\": 3}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ce97e-7dc5-4725-bd5a-4d2b2b5ae81c",
   "metadata": {},
   "source": [
    "### Stop the deployed model\n",
    "\n",
    "Use the `stop prediction` command to stop ML model serving when you have completed the prediction requests. This step is optional, you may choose to leave the model deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7cee31-e0c2-492d-bb27-8dfc96b9a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop prediction trip_dl_deploy_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aizen",
   "language": "python",
   "name": "Aizen (kernel)"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
